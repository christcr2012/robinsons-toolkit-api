/**
 * Ollama Client
 *
 * Manages connections to local Ollama instance and model selection.
 * Now with auto-start capability to save Augment credits!
 * Uses shared-llm for reliable connectivity with timeout/retry.
 */

import { Ollama } from 'ollama';
import { spawn } from 'child_process';
import { ollamaGenerate as sharedGenerate, pingOllama } from '@robinson_ai_systems/shared-llm';
import { getModelManager, type ModelSelectionCriteria } from './utils/model-manager.js';

export interface ModelConfig {
  name: string;
  speed: 'fast' | 'medium' | 'slow';
  quality: 'good' | 'better' | 'best';
  useCase: string[];
}

export interface GenerateOptions {
  model?: string;
  complexity?: 'simple' | 'medium' | 'complex';
  temperature?: number;
  maxTokens?: number;
}

export class OllamaClient {
  private ollama: any;
  private models: Map<string, ModelConfig>;
  private baseUrl: string;
  private autoStart: boolean;
  private ollamaProcess: any = null;
  private startedByUs: boolean = false;

  constructor(autoStart: boolean = true) {
    // Support remote Ollama via environment variable
    // Local (default): http://localhost:11434
    // Remote: https://ollama.my-internal-host:11434
    this.baseUrl = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
    this.ollama = new (Ollama as any)({ host: this.baseUrl });
    this.models = this.initializeModels();
    this.autoStart = autoStart;
  }

  /**
   * Get the current Ollama base URL
   */
  getBaseUrl(): string {
    return this.baseUrl;
  }

  private initializeModels(): Map<string, ModelConfig> {
    return new Map([
      // Fast router models (3B) - for intent & scaffolding
      [
        'qwen2.5:3b',
        {
          name: 'qwen2.5:3b',
          speed: 'fast',
          quality: 'good',
          useCase: ['router', 'intent', 'scaffolding', 'simple'],
        },
      ],
      [
        'llama3.2:3b',
        {
          name: 'llama3.2:3b',
          speed: 'fast',
          quality: 'good',
          useCase: ['router', 'intent', 'scaffolding', 'simple'],
        },
      ],
      // Big coder models (32-34B) - for complex tasks
      [
        'deepseek-coder:33b',
        {
          name: 'deepseek-coder:33b',
          speed: 'slow',
          quality: 'best',
          useCase: ['complex', 'algorithms', 'architecture'],
        },
      ],
      [
        'qwen2.5-coder:32b',
        {
          name: 'qwen2.5-coder:32b',
          speed: 'fast',
          quality: 'good',
          useCase: ['simple', 'crud', 'boilerplate'],
        },
      ],
      [
        'codellama:34b',
        {
          name: 'codellama:34b',
          speed: 'medium',
          quality: 'better',
          useCase: ['medium', 'refactoring', 'tests'],
        },
      ],
    ]);
  }

  /**
   * Select the best model for the task
   */
  async selectModel(options: GenerateOptions): Promise<string> {
    const modelManager = getModelManager(this.baseUrl);

    // If model explicitly specified, use it (but verify it exists)
    if (options.model && options.model !== 'auto') {
      const modelMap: Record<string, string> = {
        'router': 'qwen2.5:3b',
        'router-alt': 'llama3.2:3b',
        'deepseek-coder': 'deepseek-coder:1.3b',
        'qwen-coder': 'qwen2.5-coder:7b',
        'codellama': 'qwen2.5-coder:7b',
      };
      const requestedModel = modelMap[options.model] || options.model;

      // Check if model exists
      const modelInfo = modelManager.getModelInfo(requestedModel);
      if (modelInfo) {
        return requestedModel;
      }

      // Fall through to auto-selection if model doesn't exist
      console.warn(`[OllamaClient] Requested model ${requestedModel} not found, auto-selecting...`);
    }

    // Auto-select using ModelManager
    const criteria: ModelSelectionCriteria = {
      task: 'code', // Default to code task
      complexity: (options.complexity as any) || 'simple',
      preferSpeed: options.complexity === 'simple',
      requiredCapabilities: ['code'],
    };

    const selectedModel = await modelManager.selectModel(criteria);

    if (!selectedModel) {
      throw new Error('No Ollama models available. Please pull at least one model.');
    }

    return selectedModel;
  }

  /**
   * Generate text using Ollama (with auto-start, dynamic model selection, and adaptive timeouts!)
   */
  async generate(prompt: string, options: GenerateOptions = {}): Promise<{
    text: string;
    model: string;
    tokensGenerated: number;
    tokensInput: number;
    tokensTotal: number;
    timeMs: number;
  }> {
    // Auto-start Ollama if needed (saves Augment credits!)
    await this.ensureRunning();

    const modelManager = getModelManager(this.baseUrl);

    // Discover models if not done yet
    await modelManager.discoverModels();

    // Select best model
    const model = await this.selectModel(options);
    const startTime = Date.now();

    // Get adaptive timeout based on model size
    // Don't assume cold start - models are usually warm after first use
    const isColdStart = false;
    const timeout = await modelManager.getAdaptiveTimeout(model, isColdStart);

    console.error(`[OllamaClient] Using model: ${model} (timeout: ${timeout}ms)`);
    console.error(`[OllamaClient] Prompt length: ${prompt.length} chars`);

    try {
      // Use shared client with adaptive timeout and retry
      console.error('[OllamaClient] Calling sharedGenerate...');
      const text = await sharedGenerate({
        model,
        prompt,
        format: 'text',
        timeoutMs: timeout,
        retries: 2
      });

      const timeMs = Date.now() - startTime;
      console.error(`[OllamaClient] sharedGenerate completed in ${timeMs}ms`);

      // Estimate tokens (rough approximation: 1 token ‚âà 4 chars)
      const tokensInput = Math.ceil(prompt.length / 4);
      const tokensGenerated = Math.ceil(text.length / 4);

      return {
        text,
        model,
        tokensGenerated,
        tokensInput,
        tokensTotal: tokensInput + tokensGenerated,
        timeMs,
      };
    } catch (error: any) {
      // If model not found, try fallback
      if (error.message?.includes('not found') || error.message?.includes('404')) {
        console.warn(`[OllamaClient] Model ${model} failed, trying fallback...`);

        const criteria: ModelSelectionCriteria = {
          task: 'code',
          complexity: (options.complexity as any) || 'simple',
        };

        const fallbackChain = await modelManager.getFallbackChain(model, criteria);

        // Try first fallback
        if (fallbackChain.length > 1) {
          const fallbackModel = fallbackChain[1];
          console.error(`[OllamaClient] Retrying with fallback model: ${fallbackModel}`);

          const fallbackTimeout = await modelManager.getAdaptiveTimeout(fallbackModel, isColdStart);

          const text = await sharedGenerate({
            model: fallbackModel,
            prompt,
            format: 'text',
            timeoutMs: fallbackTimeout,
            retries: 1
          });

          const timeMs = Date.now() - startTime;
          const tokensInput = Math.ceil(prompt.length / 4);
          const tokensGenerated = Math.ceil(text.length / 4);

          return {
            text,
            model: fallbackModel,
            tokensGenerated,
            tokensInput,
            tokensTotal: tokensInput + tokensGenerated,
            timeMs,
          };
        }

        throw new Error(
          `No available models found. Please pull at least one model: ollama pull qwen2.5:3b`
        );
      }
      throw error;
    }
  }

  /**
   * Check if Ollama is running and models are available
   */
  async checkHealth(): Promise<{
    running: boolean;
    models: string[];
    errors: string[];
  }> {
    const errors: string[] = [];
    let running = false;
    let availableModels: string[] = [];

    try {
      // Check if Ollama is running
      const response = await this.ollama.list();
      running = true;
      availableModels = response.models.map((m: any) => m.name);

      // Check if recommended models are available
      const recommendedModels = [
        'qwen2.5:3b',           // Fast router
        'deepseek-coder:33b',   // Best quality
        'qwen2.5-coder:32b',    // Fast coder
        'codellama:34b',        // Balanced
      ];

      for (const model of recommendedModels) {
        if (!availableModels.includes(model)) {
          errors.push(`Model ${model} not found. Run: ollama pull ${model}`);
        }
      }
    } catch (error: any) {
      errors.push(`Ollama not running. Please start Ollama.`);
    }

    return {
      running,
      models: availableModels,
      errors,
    };
  }

  /**
   * Get model info
   */
  getModelInfo(modelName: string): ModelConfig | undefined {
    return this.models.get(modelName);
  }

  /**
   * List all configured models
   */
  listModels(): ModelConfig[] {
    return Array.from(this.models.values());
  }

  /**
   * Auto-start Ollama if not running (saves Augment credits!)
   * Enhanced with better detection, configurable timeout, and exponential backoff
   */
  private async startOllama(): Promise<void> {
    console.error('üöÄ Auto-starting Ollama...');

    // Get configurable timeout (default: 120 seconds, increased from 60)
    const timeoutSeconds = parseInt(process.env.OLLAMA_START_TIMEOUT || '120', 10);
    console.error(`‚è±Ô∏è Using timeout: ${timeoutSeconds} seconds`);

    // Get Ollama path from environment or use defaults
    const ollamaPath = process.env.OLLAMA_PATH || (
      process.platform === 'win32'
        ? 'C:\\Users\\chris\\AppData\\Local\\Programs\\Ollama\\ollama.exe'
        : 'ollama'
    );

    try {
      // First, check if Ollama is already running (might be Windows service)
      console.error('üîç Checking if Ollama is already running...');
      console.error(`üîó Base URL: ${this.baseUrl}`);
      const isRunning = await pingOllama(5000); // Increased timeout for initial check

      if (isRunning) {
        console.error('‚úÖ Ollama is already running!');
        return;
      }

      console.error('‚ùå Ollama not responding, attempting to start...');

      console.error(`üöÄ Spawning Ollama process: ${ollamaPath}`);

      this.ollamaProcess = spawn(ollamaPath, ['serve'], {
        detached: true,
        stdio: 'ignore',
        windowsHide: true
      });

      this.ollamaProcess.unref();
      this.startedByUs = true;

      console.error(`‚è≥ Waiting for Ollama to be ready (timeout: ${timeoutSeconds}s)...`);

      // Exponential backoff: 1s, 2s, 4s, 8s, then 1s intervals
      const delays = [1000, 2000, 4000, 8000];
      let totalWait = 0;
      let attemptCount = 0;

      while (totalWait < timeoutSeconds * 1000) {
        const delay = attemptCount < delays.length ? delays[attemptCount] : 1000;
        await new Promise(resolve => setTimeout(resolve, delay));
        totalWait += delay;
        attemptCount++;

        try {
          const ready = await pingOllama(2000);
          if (ready) {
            console.error(`‚úÖ Ollama ready after ${totalWait}ms!`);
            return;
          }
        } catch {
          // Not ready yet, continue waiting
          console.error(`‚è≥ Still waiting... (${Math.floor(totalWait / 1000)}s / ${timeoutSeconds}s)`);
        }
      }

      throw new Error(`Ollama started but not ready within ${timeoutSeconds} seconds. Try increasing OLLAMA_START_TIMEOUT.`);
    } catch (error: any) {
      // Better error messages
      if (error.code === 'ENOENT') {
        throw new Error(
          `Ollama not found at: ${ollamaPath}\n` +
          `Please install Ollama from https://ollama.com or set OLLAMA_PATH environment variable.`
        );
      }

      if (error.code === 'EADDRINUSE' || error.message?.includes('address already in use')) {
        throw new Error(
          `Port 11434 is already in use. Another Ollama instance may be running.\n` +
          `Try: pkill ollama (Linux/Mac) or taskkill /F /IM ollama.exe (Windows)`
        );
      }

      throw new Error(`Failed to auto-start Ollama: ${error.message}`);
    }
  }

  /**
   * Ensure Ollama is running (auto-start if needed)
   * Enhanced with better health checking using pingOllama
   */
  async ensureRunning(): Promise<void> {
    console.error(`[OllamaClient] Ensuring Ollama is running at ${this.baseUrl}...`);

    try {
      // Use pingOllama for more reliable health check with longer timeout
      console.error('[OllamaClient] Checking Ollama health...');
      const isRunning = await pingOllama(10000); // 10 second timeout

      if (isRunning) {
        console.error('[OllamaClient] ‚úÖ Ollama is running and healthy!');
        return; // Already running, all good!
      }

      console.error('[OllamaClient] ‚ùå Ollama not responding');

      // Not running, try to start if auto-start enabled
      if (this.autoStart) {
        console.error('[OllamaClient] Auto-start enabled, attempting to start Ollama...');
        await this.startOllama();

        // Verify it started successfully
        const isNowRunning = await pingOllama(5000);
        if (!isNowRunning) {
          throw new Error('Ollama started but still not responding to health checks');
        }
        console.error('[OllamaClient] ‚úÖ Ollama started successfully!');
      } else {
        throw new Error(
          'Ollama is not running. Please start Ollama with: ollama serve\n' +
          'Or enable auto-start by setting autoStart=true in constructor.'
        );
      }
    } catch (error: any) {
      console.error(`[OllamaClient] Error in ensureRunning: ${error.message}`);

      // If pingOllama failed and we haven't tried auto-start yet, try it
      if (this.autoStart && !error.message?.includes('auto-start') && !error.message?.includes('started but still not responding')) {
        console.error('[OllamaClient] Ping failed, trying auto-start as fallback...');
        await this.startOllama();
      } else {
        throw error;
      }
    }
  }

  /**
   * Cleanup spawned Ollama process on shutdown
   */
  async cleanup(): Promise<void> {
    if (this.ollamaProcess && this.startedByUs) {
      console.error('üßπ Cleaning up spawned Ollama process...');
      try {
        this.ollamaProcess.kill();
        console.error('‚úÖ Ollama process terminated');
      } catch (error: any) {
        console.error(`‚ö†Ô∏è Failed to kill Ollama process: ${error.message}`);
      }
    }
  }
}

